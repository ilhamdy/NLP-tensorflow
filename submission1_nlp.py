# -*- coding: utf-8 -*-
"""Submission1_NLP.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YPObGqVyJRFdI8F4QHwrgnomrviSmYoc

# Membuat Model NPL dengan Tensorflow

**Data Diri**
*   Nama : Ilham Dwi Yanto
*   Domisili : Madiun, Jawa Timur
*   Email : ilham.dy18@gmail.com
"""

import numpy as np 
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, SpatialDropout1D, Dense
from keras.utils.np_utils import to_categorical
from tensorflow.keras.callbacks import EarlyStopping

df = pd.read_csv('/content/bitcoin_articles.csv', usecols=['title', 'topic'])
df.head()

df.shape

df.topic.value_counts()

df.topic.loc[(df['topic'] == "economics") | (df['topic'] == "world") | (df['topic'] == "entertainment") | 
             (df['topic'] == "science") | (df['topic'] == "beauty") | (df['topic'] == "politics") | 
             (df['topic'] == "sport") | (df['topic'] == "food") | (df['topic'] == "energy") | 
             (df['topic'] == "tech")] = "others topic"

df.reset_index(inplace = True)
df.drop('index', axis='columns', inplace=True)

df.topic.value_counts()

num_word = 50000

ubah_df = df.reindex(np.random.permutation(df.index))

news = ubah_df[ubah_df['topic'] == 'news'][:num_word]
finance = ubah_df[ubah_df['topic'] == 'finance'][:num_word]
business = ubah_df[ubah_df['topic'] == 'business'][:num_word]
others_topic = ubah_df[ubah_df['topic'] == 'others topic'][:num_word]

df_baru = pd.concat([news, finance, business, others_topic], ignore_index=True)

df_baru = df_baru.reindex(np.random.permutation(df_baru.index))
df_baru['label'] = 0

df_baru.loc[df_baru['topic'] == 'news', 'label'] = 0
df_baru.loc[df_baru['topic'] == 'finance', 'label'] = 1
df_baru.loc[df_baru['topic'] == 'business', 'label'] = 2
df_baru.loc[df_baru['topic'] == 'others topic', 'label'] = 3

print(df_baru['label'][:10])

labels = to_categorical(df_baru['label'], num_classes=4)

print(labels[:10])

max_words = 5000
max_len = 150

tokenizer = Tokenizer(num_words=max_words, filters='!"#$%&()*+,-./:;<=>?@[\]^_`{|}~', oov_token='x', lower=True)
tokenizer.fit_on_texts(df_baru['title'].values)
sequences = tokenizer.texts_to_sequences(df_baru['title'].values)
word_index = tokenizer.word_index
print('Ditemukan %s tokens unik.' % len(word_index))

X = pad_sequences(sequences, maxlen=max_len, padding='post', truncating='post')

X_train, X_test, y_train, y_test = train_test_split(X , labels, test_size=0.25, random_state=42)

print((X_train.shape, y_train.shape, X_test.shape, y_test.shape))

epochs = 20
emb_dim = 128
batch_size = 64

labels[:10]

model = Sequential()
model.add(Embedding(max_words, emb_dim, input_length=X.shape[1]))
model.add(SpatialDropout1D(0.7))
model.add(LSTM(64, dropout=0.7, recurrent_dropout=0.7))
model.add(Dense(4, activation='softmax'))

model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

print(model.summary())

history = model.fit(X_train, y_train, 
                    epochs=epochs, batch_size=batch_size,
                    validation_split=0.2,
                    callbacks=[EarlyStopping(monitor='val_loss', patience=5, min_delta=0.0001)])

accuracy = model.evaluate(X_test, y_test)
print('Test set\n  Loss: {:0.3f}\n  Accuracy: {:0.3f}'.format(accuracy[0],accuracy[1]))

import matplotlib.pyplot as plt

accu = history.history['accuracy']
val_accu = history.history['val_accuracy']
loss = history.history['loss']
val_loss = history.history['val_loss']

epochs = range(1, len(accu) + 1)

plt.plot(epochs, accu, 'bo', label='Training accuracy')
plt.plot(epochs, val_accu, 'b', label='Validation accuracy')
plt.title('Training and validation accuracy')
plt.legend()

plt.figure()

plt.plot(epochs, loss, 'bo', label='Training loss')
plt.plot(epochs, val_loss, 'b', label='Validation loss')
plt.title('Training and validation loss')
plt.legend()

plt.show()

ex_title = ["The Orange Party Issue Playlist"]
labels = ['news', 'bussiness', 'finance', 'others topic']

seq = tokenizer.texts_to_sequences(ex_title)
padded = pad_sequences(seq, maxlen=max_len)
pred = model.predict(padded)

print(pred, labels[np.argmax(pred)])